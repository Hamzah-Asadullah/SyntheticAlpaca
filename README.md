# SyntheticAlpaca
A lightweight script to generate and scrape LM text generations. Getting up and running is easy, just run main.py (can have a look at it if you're paranoid) and it'll guide you through.  

## Published Generations

Alpaca styled:

- [Hamzah-Asadullah/TA-Shiki-2k](https://huggingface.co/datasets/Hamzah-Asadullah/TA-Shiki-2k) (deleted)
- [Hamzah-Asadullah/TA-4k](https://huggingface.co/datasets/Hamzah-Asadullah/TA-4k) (deleted)
- [Hamzah-Asadullah/TA-WQS-8k](https://hf.co/datasets/Hamzah-Asadullah/TA-WQS-8k) (deleted)
- [XeTute/TA-Engineering-4k](https://huggingface.co/datasets/XeTute/TA-Engineering-4k) (deleted)
- [XeTute/TA-8k](https://huggingface.co/datasets/XeTute/TA-8k) (deleted)
- [XeTute/TA-SS-15k](https://huggingface.co/datasets/XeTute/TA-SS-15k) (deleted)

You can find a merged `data.json` file of all deleted [here](https://huggingface.co/Hamzah-Asadullah/Failed-FPFT-0.6B/resolve/main/data.json).  
ShareGPT-ish styled (they're larger in file size):

- [Hamzah-Asadullah/TypaRP-16x1k](https://huggingface.co/datasets/Hamzah-Asadullah/TypaRP-16x1k)
- [Hamzah-Asadullah/Vibin-GPT4o](https://huggingface.co/datasets/Hamzah-Asadullah/Vibin-GPT4o) (deleted)
- [XeTute/TypaRP-12x2k](https://huggingface.co/datasets/XeTute/TypaRP-12x2k)

In total, that leads to ~44k reported samples generated using SyntheticAlpaca. If you want your dataset to be displayed here, you may open an GitHub issue.  
**Happy coding.**
